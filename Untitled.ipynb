{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3cbaad3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gradio'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Third-party library imports\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfaster_whisper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WhisperModel\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgr\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hf_hub_download\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gradio'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Set CUDA environment variable and install llama-cpp-python\n",
    "# llama-cpp-python is a python binding for llama.cpp library which enables LLM inference in pure C/C++\n",
    "os.environ[\"CUDACXX\"] = \"/usr/local/cuda/bin/nvcc\"\n",
    "os.system('python -m unidic download')\n",
    "os.system('CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python==0.2.11 --verbose')\n",
    "\n",
    "\n",
    "# Third-party library imports\n",
    "from fasterwhisper import WhisperModel\n",
    "import gradio as gr\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "from TTS.tts.configs.xtts_config import XttsConfig\n",
    "from TTS.tts.models.xtts import Xtts\n",
    "from TTS.utils.generic_utils import get_user_data_dir\n",
    "from TTS.utils.manage import ModelManager\n",
    "\n",
    "# Local imports\n",
    "from utils import get_sentence, generate_speech_for_sentence, wave_header_chunk\n",
    "\n",
    "# Load Whisper ASR model\n",
    "print(\"Loading Whisper ASR\")\n",
    "whisper_model = WhisperModel(\"large-v3\", device=\"cuda\", compute_type=\"float16\")\n",
    "\n",
    "# Load Mistral LLM\n",
    "print(\"Loading Mistral LLM\")\n",
    "hf_hub_download(repo_id=\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\", local_dir=\".\", filename=\"mistral-7b-instruct-v0.1.Q5_K_M.gguf\")\n",
    "mistral_model_path=\"./mistral-7b-instruct-v0.1.Q5_K_M.gguf\"\n",
    "mistral_llm = Llama(model_path=mistral_model_path,n_gpu_layers=35,max_new_tokens=256, context_window=4096, n_ctx=4096,n_batch=128,verbose=False)\n",
    "\n",
    "\n",
    "# Load XTTS Model\n",
    "print(\"Loading XTTS model\")\n",
    "os.environ[\"COQUI_TOS_AGREED\"] = \"1\"\n",
    "tts_model_name = \"tts_models/multilingual/multi-dataset/xtts_v2\"\n",
    "ModelManager().download_model(tts_model_name)\n",
    "tts_model_path = os.path.join(get_user_data_dir(\"tts\"), tts_model_name.replace(\"/\", \"--\"))\n",
    "config = XttsConfig()\n",
    "config.load_json(os.path.join(tts_model_path, \"config.json\"))\n",
    "xtts_model = Xtts.init_from_config(config)\n",
    "xtts_model.load_checkpoint(\n",
    "    config,\n",
    "    checkpoint_path=os.path.join(tts_model_path, \"model.pth\"),\n",
    "    vocab_path=os.path.join(tts_model_path, \"vocab.json\"),\n",
    "    eval=True,\n",
    "    use_deepspeed=True,\n",
    ")\n",
    "xtts_model.cuda()\n",
    "\n",
    "###### Set up Gradio Interface ######\n",
    "\n",
    "with gr.Blocks(title=\"Voice chat with LLM\") as demo:\n",
    "    DESCRIPTION = \"\"\"# Voice chat with LLM\"\"\"\n",
    "    gr.Markdown(DESCRIPTION)\n",
    "\n",
    "    # Define chatbot component\n",
    "    chatbot = gr.Chatbot(\n",
    "        value=[(None, \"Hi friend, I'm Amy, an AI coach. How can I help you today?\")],  # Initial greeting from the chatbot\n",
    "        elem_id=\"chatbot\",\n",
    "        avatar_images=(\"examples/hf-logo.png\", \"examples/ai-chat-logo.png\"),\n",
    "        bubble_full_width=False,\n",
    "    )\n",
    "\n",
    "    # Define chatbot voice component\n",
    "    VOICES = [\"female\", \"male\"]\n",
    "    with gr.Row():\n",
    "        chatbot_voice = gr.Dropdown(\n",
    "            label=\"Voice of the Chatbot\",\n",
    "            info=\"How should Chatbot talk like\",\n",
    "            choices=VOICES,\n",
    "            max_choices=1,\n",
    "            value=VOICES[0],\n",
    "        )\n",
    "\n",
    "    # Define text and audio record input components\n",
    "    with gr.Row():\n",
    "        txt_box = gr.Textbox(\n",
    "            scale=3,\n",
    "            show_label=False,\n",
    "            placeholder=\"Enter text and press enter, or speak to your microphone\",\n",
    "            container=False,\n",
    "            interactive=True,\n",
    "        )\n",
    "        audio_record = gr.Audio(source=\"microphone\", type=\"filepath\", scale=4)\n",
    "\n",
    "    # Define generated audio playback component \n",
    "    with gr.Row():\n",
    "        sentence = gr.Textbox(visible=False)\n",
    "        audio_playback = gr.Audio(\n",
    "            value=None,\n",
    "            label=\"Generated audio response\",\n",
    "            streaming=True,\n",
    "            autoplay=True,\n",
    "            interactive=False,\n",
    "            show_label=True,\n",
    "        )\n",
    "\n",
    "    # Will be triggered on text submit (will send to generate_speech)\n",
    "    def add_text(chatbot_history, text):\n",
    "        chatbot_history = [] if chatbot_history is None else chatbot_history\n",
    "        chatbot_history = chatbot_history + [(text, None)]\n",
    "        return chatbot_history, gr.update(value=\"\", interactive=False)\n",
    "    \n",
    "    # Will be triggered on voice submit (will transribe and send to generate_speech)\n",
    "    def add_audio(chatbot_history, audio):\n",
    "        chatbot_history = [] if chatbot_history is None else chatbot_history\n",
    "        # get result from whisper and strip it to delete begin and end space\n",
    "        response, _ = whisper_model.transcribe(audio)\n",
    "        text = list(response)[0].text.strip()\n",
    "        print(\"Transcribed text:\", text)\n",
    "        chatbot_history = chatbot_history + [(text, None)]\n",
    "        return chatbot_history, gr.update(value=\"\", interactive=False)\n",
    "\n",
    "    def generate_speech(chatbot_history, chatbot_voice, initial_greeting=False):\n",
    "        # Start by yielding an initial empty audio to set up autoplay\n",
    "        yield (\"\", chatbot_history, wave_header_chunk())\n",
    "\n",
    "        # Helper function to handle the speech generation and yielding process\n",
    "        def handle_speech_generation(sentence, chatbot_history, chatbot_voice):\n",
    "            if sentence != \"\":\n",
    "                print(\"Processing sentence\")\n",
    "                generated_speech = generate_speech_for_sentence(chatbot_history, chatbot_voice, sentence, xtts_model, xtts_supported_languages=config.languages, return_as_byte=True)\n",
    "                if generated_speech is not None:\n",
    "                    _, audio_dict = generated_speech\n",
    "                    yield (sentence, chatbot_history, audio_dict[\"value\"])\n",
    "\n",
    "        if initial_greeting:\n",
    "            # Process only the initial greeting if specified\n",
    "            for _, sentence in chatbot_history:\n",
    "                yield from handle_speech_generation(sentence, chatbot_history, chatbot_voice)\n",
    "        else:\n",
    "            # Continuously get and process sentences from a generator function\n",
    "            for sentence, chatbot_history in get_sentence(chatbot_history, mistral_llm):\n",
    "                print(\"Inserting sentence to queue\")\n",
    "                yield from handle_speech_generation(sentence, chatbot_history, chatbot_voice)\n",
    "\n",
    "    txt_msg = txt_box.submit(fn=add_text, inputs=[chatbot, txt_box], outputs=[chatbot, txt_box], queue=False\n",
    "                             ).then(fn=generate_speech,  inputs=[chatbot,chatbot_voice], outputs=[sentence, chatbot, audio_playback])\n",
    "\n",
    "    txt_msg.then(fn=lambda: gr.update(interactive=True), inputs=None, outputs=[txt_box], queue=False)\n",
    "\n",
    "    audio_msg = audio_record.stop_recording(fn=add_audio, inputs=[chatbot, audio_record], outputs=[chatbot, txt_box], queue=False\n",
    "                                            ).then(fn=generate_speech,  inputs=[chatbot,chatbot_voice], outputs=[sentence, chatbot, audio_playback])\n",
    "\n",
    "    audio_msg.then(fn=lambda: (gr.update(interactive=True),gr.update(interactive=True,value=None)), inputs=None, outputs=[txt_box, audio_record], queue=False)\n",
    "\n",
    "    FOOTNOTE = \"\"\"\n",
    "            This Space demonstrates how to speak to an llm chatbot, based solely on open accessible models.\n",
    "            It relies on the following models :\n",
    "            - Speech to Text Model: [Faster-Whisper-large-v3](https://huggingface.co/Systran/faster-whisper-large-v3) an ASR model, to transcribe recorded audio to text.\n",
    "            - Large Language Model: [Mistral-7b-instruct-v0.1-quantized](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF) a LLM to generate the chatbot responses. \n",
    "            - Text to Speech Model: [XTTS-v2](https://huggingface.co/spaces/coqui/xtts) a TTS model, to generate the voice of the chatbot.\n",
    "\n",
    "            Note:\n",
    "            - Responses generated by chat model should not be assumed correct or taken serious, as this is a demonstration example only\n",
    "            - iOS (Iphone/Ipad) devices may not experience voice due to autoplay being disabled on these devices by Vendor\"\"\"\n",
    "    gr.Markdown(FOOTNOTE)\n",
    "    demo.load(fn=generate_speech, inputs=[chatbot,chatbot_voice, gr.State(value=True)], outputs=[sentence, chatbot, audio_playback])\n",
    "demo.queue().launch(debug=True,share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d807572",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed57bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
